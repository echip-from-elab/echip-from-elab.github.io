---
layout: post
read_time: true
show_date: true
title: "First chiplet for mamba-based nerual networks in GF 22nm technology"
date: 2025-05-29
img: posts/20210324/mamba_ssm.gif
tags: [state space model, low computation complexity, edge-computing]
author: Miao Sun
description: "Midlife career change: a disaster or an opportunity?"
---

State Space Model (SSM)-based machine learning architectures have recently gained significant attention for processing sequential data. For a long time, Transformer models dominated sequence modeling due to their powerful capabilities. However, as model scales continuously expand, the Transformers' inherent quadratic complexity leads to significant computational resource and energy consumption overhead, which has become a bottleneck for their further development.

Against this backdrop, Mamba, a recently introduced sequence-to-sequence SSM architecture, demonstrates superior capabilities beyond existing model frameworks. It achieves comparable accuracy to Transformer models on various tasks while significantly improving computational efficiency, substantially reducing training and inference costs.

| Syntax      | Description | Test Text     |
| :---        |    :----:   |          ---: |
| Header      | Title       | Here's this   |
| Paragraph   | Text        | And more      |